{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f301c29a",
   "metadata": {},
   "source": [
    "### Word-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f9c6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37165085",
   "metadata": {},
   "source": [
    "### Character-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35014f5f",
   "metadata": {},
   "source": [
    "### Subword-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08cbfb",
   "metadata": {},
   "source": [
    "### Loading and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc3bdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaellee/anaconda3/envs/openai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26285907",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d050537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea280d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)/main/tokenizer.json: 100%|███| 436k/436k [00:00<00:00, 21.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1f9b486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5158be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizers/tokenizer_config.json',\n",
       " 'tokenizers/special_tokens_map.json',\n",
       " 'tokenizers/vocab.txt',\n",
       " 'tokenizers/added_tokens.json',\n",
       " 'tokenizers/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"tokenizers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37106888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|█████| 481/481 [00:00<00:00, 1.59MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|███| 899k/899k [00:00<00:00, 46.6MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|███| 456k/456k [00:00<00:00, 37.3MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|█| 1.36M/1.36M [00:00<00:00, 48.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'Ġa', 'ĠTrans', 'former', 'Ġnetwork', 'Ġis', 'Ġsimple']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31b24239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36949, 10, 5428, 22098, 1546, 16, 2007]\n"
     ]
    }
   ],
   "source": [
    "## from tokens to input ids\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c5bbc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0975ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cba9149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "480c047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the best source for learning Dynamic Programming?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2553daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb41359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13ce3287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2264,    16,     5,   275,  1300,    13,  2239, 29614, 39538,   116])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd855e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  2264,    16,     5,   275,  1300,    13,  2239, 29614, 39538,\n",
      "           116,     2]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3657b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 2264,    16,     5,   275,  1300,    13,  2239, 29614, 39538,   116]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "checkpoint = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
    "\n",
    "query = \"What is the best source for learning Dynamic Programming?\"\n",
    "\n",
    "tokens = tokenizer.tokenize(query)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89d776d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-2.5217, -8.2146, -8.2114, -7.8768, -7.6339, -8.3176, -8.1205, -8.2073,\n",
      "         -8.5701, -8.3297]], grad_fn=<CloneBackward0>), end_logits=tensor([[-1.5421, -7.0788, -7.2135, -6.8254, -5.6122, -6.7535, -6.7843, -6.9499,\n",
      "         -6.2422, -6.3289]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b930ebef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-0.9507, -6.2646, -6.0937]], grad_fn=<CloneBackward0>), end_logits=tensor([[-0.2857, -4.8904, -4.6521]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n",
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-2.2159, -6.9054]], grad_fn=<CloneBackward0>), end_logits=tensor([[-1.3976, -5.6106]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n",
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-0.9507, -6.2646, -6.0937],\n",
      "        [-1.8093, -6.3697, -5.8797]], grad_fn=<CloneBackward0>), end_logits=tensor([[-0.2857, -4.8905, -4.6521],\n",
      "        [-0.8618, -4.7892, -4.4480]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)))\n",
    "print(model(torch.tensor(sequence2_ids)))\n",
    "print(model(torch.tensor(batched_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82e14d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-0.9507, -6.2646, -6.0937],\n",
      "        [-2.2159, -6.9054, -6.8160]], grad_fn=<CloneBackward0>), end_logits=tensor([[-0.2857, -4.8905, -4.6521],\n",
      "        [-1.3976, -5.6106, -5.5207]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2c9656b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-0.9507, -6.2646, -6.0937]], grad_fn=<CloneBackward0>), end_logits=tensor([[-0.2857, -4.8904, -4.6521]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(model(torch.tensor(sequence1_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf8c6754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-2.2159, -6.9054]], grad_fn=<CloneBackward0>), end_logits=tensor([[-1.3976, -5.6106]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(model(torch.tensor(sequence2_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c80ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
